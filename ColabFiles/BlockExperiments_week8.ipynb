{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMdGp2weTcWTDSwm8w/0SQ/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KORALLLL/MTUCI_EMNIST/blob/Sasha/ColabFiles/BlockExperiments_week8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FW4XTjXctzBv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import torchvision.datasets\n",
        "import torchvision.transforms as transforms\n",
        "from tqdm import tqdm_notebook as bar\n",
        "import pickle\n",
        "import torchvision.datasets\n",
        "\n",
        "random.seed(0)\n",
        "np.random.seed(0)\n",
        "torch.cuda.manual_seed(0)\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/KORALLLL/MTUCI_EMNIST.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAZCwD7yyQ8D",
        "outputId": "0ea877cd-d2b1-4197-c82b-fdd8a22b6e8f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MTUCI_EMNIST'...\n",
            "remote: Enumerating objects: 36640, done.\u001b[K\n",
            "remote: Counting objects: 100% (13000/13000), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12827/12827), done.\u001b[K\n",
            "remote: Total 36640 (delta 284), reused 12808 (delta 160), pack-reused 23640\u001b[K\n",
            "Receiving objects: 100% (36640/36640), 137.20 MiB | 14.36 MiB/s, done.\n",
            "Resolving deltas: 100% (569/569), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_optimizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5P9RY5I_EHR",
        "outputId": "b1b2bb9c-97fc-4234-ec2b-e70738b187d5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_optimizer\n",
            "  Downloading torch_optimizer-0.3.0-py3-none-any.whl (61 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/61.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.9/61.9 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from torch_optimizer) (2.1.0+cu121)\n",
            "Collecting pytorch-ranger>=0.1.1 (from torch_optimizer)\n",
            "  Downloading pytorch_ranger-0.1.1-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->torch_optimizer) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->torch_optimizer) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->torch_optimizer) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->torch_optimizer) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->torch_optimizer) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->torch_optimizer) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->torch_optimizer) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.5.0->torch_optimizer) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.5.0->torch_optimizer) (1.3.0)\n",
            "Installing collected packages: pytorch-ranger, torch_optimizer\n",
            "Successfully installed pytorch-ranger-0.1.1 torch_optimizer-0.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_optimizer import AdaBound"
      ],
      "metadata": {
        "id": "o_V6pGEu_FEz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emnist_train = torchvision.datasets.EMNIST('./', split='letters', download = True, train = True)\n",
        "mnist_train = torchvision.datasets.EMNIST('./', split='mnist', download = True, train = True)\n",
        "emnist_val = torchvision.datasets.EMNIST('./', split='letters', download = True, train = False)\n",
        "mnist_val = torchvision.datasets.EMNIST('./', split='mnist', download = True, train = False)\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "emnist_train_data = emnist_train.data\n",
        "mnist_train_data = mnist_train.data\n",
        "train_data = torch.cat([emnist_train_data[:24960], mnist_train_data[:12000]], dim=0).float().unsqueeze(1).to(device) / 255\n",
        "\n",
        "emnist_val_data = emnist_val.data\n",
        "mnist_val_data = mnist_val.data\n",
        "val_data = torch.cat([emnist_val_data, mnist_val_data], dim=0).float().unsqueeze(1).to(device) / 255\n",
        "\n",
        "temp_emnist_train_labels = emnist_train.targets[:24960]\n",
        "emnist_train_labels = []\n",
        "\n",
        "temp_emnist_val_labels = emnist_val.targets\n",
        "emnist_val_labels = []\n",
        "\n",
        "for i in range(len(temp_emnist_train_labels)):\n",
        "  if temp_emnist_train_labels[i]==15:\n",
        "    emnist_train_labels.append(torch.tensor(0))\n",
        "  elif temp_emnist_train_labels[i]>15:\n",
        "    emnist_train_labels.append(temp_emnist_train_labels[i]+8)\n",
        "  else:\n",
        "    emnist_train_labels.append(temp_emnist_train_labels[i]+9)\n",
        "\n",
        "for i in range(len(temp_emnist_val_labels)):\n",
        "  if temp_emnist_val_labels[i]==15:\n",
        "    emnist_val_labels.append(torch.tensor(0))\n",
        "  elif temp_emnist_val_labels[i]>15:\n",
        "    emnist_val_labels.append(temp_emnist_val_labels[i] + 8)\n",
        "  else:\n",
        "    emnist_val_labels.append(temp_emnist_val_labels[i]+9)\n",
        "\n",
        "mnist_train_labels = mnist_train.targets\n",
        "train_labels = torch.cat([torch.stack(emnist_train_labels[:24960]), mnist_train_labels[:12000]], dim=0).to(device)\n",
        "\n",
        "mnist_val_labels = mnist_val.targets\n",
        "val_labels = torch.cat([torch.stack(emnist_val_labels), mnist_val_labels], dim=0).to(device)\n",
        "\n",
        "\n",
        "file = open('MTUCI_EMNIST/dataset.pkl', 'rb')\n",
        "test_dataset = pickle.load(file)\n",
        "file.close()\n",
        "\n",
        "test_data = test_dataset['data'].numpy()\n",
        "test_data = np.flip(test_data, axis = 3)\n",
        "test_data = np.rot90(test_data, k=1, axes=(2,3))\n",
        "test_data = 1 - test_data\n",
        "test_data = torch.from_numpy(test_data).float().to(device)\n",
        "test_labels = test_dataset['targets'].to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMB9_wGtySmh",
        "outputId": "70080ee3-774a-4020-821c-995079aff3e6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.itl.nist.gov/iaui/vip/cs_links/EMNIST/gzip.zip to ./EMNIST/raw/gzip.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 561753746/561753746 [00:21<00:00, 25917065.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./EMNIST/raw/gzip.zip to ./EMNIST/raw\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Лучшая сеть: с дропаутом с шансом 0.1 64 эпохи обучения\n",
        "\n",
        "Добавлен 1 блок"
      ],
      "metadata": {
        "id": "JI6Qv7YbyUnd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "TBVZDjAjgRqO"
      },
      "outputs": [],
      "source": [
        "class LeNetOneBlock(torch.nn.Module):\n",
        "  def __init__(self, dropout_rate):\n",
        "    super(LeNetOneBlock, self).__init__()\n",
        "\n",
        "#beginning of the first block\n",
        "    self.conv1_a = torch.nn.Conv2d(in_channels=1, out_channels=2,\n",
        "                                 kernel_size=3, padding=1)\n",
        "    self.act1_a = torch.nn.ReLU6()\n",
        "    self.dropout1_a = torch.nn.Dropout(p=dropout_rate)\n",
        "    self.conv2_a = torch.nn.Conv2d(in_channels=2, out_channels=4,\n",
        "                                 kernel_size=3, padding=1)\n",
        "\n",
        "    self.act2_a = torch.nn.ReLU6()\n",
        "    self.dropout2_a = torch.nn.Dropout(p=dropout_rate)\n",
        "    self.conv3_a = torch.nn.Conv2d(in_channels=4, out_channels=8,\n",
        "                                 kernel_size=3, padding=1)\n",
        "    self.act3_a = torch.nn.ReLU6()\n",
        "    self.dropout3_a = torch.nn.Dropout(p=dropout_rate)\n",
        "    self.conv4_a = torch.nn.Conv2d(in_channels=8, out_channels=16,\n",
        "                                 kernel_size=3, padding=1)\n",
        "    self.act4_a = torch.nn.ReLU6()\n",
        "    self.dropout4_a = torch.nn.Dropout(p=dropout_rate)\n",
        "    self.conv5_a = torch.nn.Conv2d(in_channels=16, out_channels=32,\n",
        "                                 kernel_size=3, padding=1)\n",
        "    self.act5_a = torch.nn.ReLU6()\n",
        "    self.dropout5_a = torch.nn.Dropout(p=dropout_rate)\n",
        "#end of the first block\n",
        "\n",
        "    self.conv1 = torch.nn.Conv2d(in_channels=32, out_channels=64,\n",
        "                                 kernel_size=3, padding=1)\n",
        "    self.act1 = torch.nn.ReLU6()\n",
        "    self.dropout1 = torch.nn.Dropout(p=dropout_rate)\n",
        "    self.conv2 = torch.nn.Conv2d(in_channels=64, out_channels=128,\n",
        "                                 kernel_size=3, padding=1)\n",
        "\n",
        "    self.act2 = torch.nn.ReLU6()\n",
        "    self.dropout2 = torch.nn.Dropout(p=dropout_rate)\n",
        "    self.conv3 = torch.nn.Conv2d(in_channels=128, out_channels=256,\n",
        "                                 kernel_size=3, padding=1)\n",
        "    self.act3 = torch.nn.ReLU6()\n",
        "    self.dropout3 = torch.nn.Dropout(p=dropout_rate)\n",
        "    self.conv4 = torch.nn.Conv2d(in_channels=256, out_channels=512,\n",
        "                                 kernel_size=3, padding=1)\n",
        "    self.act4 = torch.nn.ReLU6()\n",
        "    self.dropout4 = torch.nn.Dropout(p=dropout_rate)\n",
        "    self.conv5 = torch.nn.Conv2d(in_channels=512, out_channels=512,\n",
        "                                 kernel_size=3, padding=1)\n",
        "    self.act5 = torch.nn.ReLU6()\n",
        "    self.dropout5 = torch.nn.Dropout(p=dropout_rate)\n",
        "    self.pool1 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    self.act6 = torch.nn.ReLU6()\n",
        "\n",
        "\n",
        "    self.conv6 = torch.nn.Conv2d(in_channels=512, out_channels=512,\n",
        "                                 kernel_size=3, padding=1)\n",
        "    self.act7 = torch.nn.ReLU6()\n",
        "    self.dropout6 = torch.nn.Dropout(p=dropout_rate)\n",
        "    self.conv7 = torch.nn.Conv2d(in_channels=512, out_channels=512,\n",
        "                                 kernel_size=3, padding=1)\n",
        "    self.act8 = torch.nn.ReLU6()\n",
        "    self.dropout7 = torch.nn.Dropout(p=dropout_rate)\n",
        "    self.conv8 = torch.nn.Conv2d(in_channels=512, out_channels=512,\n",
        "                                 kernel_size=3, padding=1)\n",
        "    self.act9 = torch.nn.ReLU6()\n",
        "    self.dropout8 = torch.nn.Dropout(p=dropout_rate)\n",
        "    self.conv9 = torch.nn.Conv2d(in_channels=512, out_channels=512,\n",
        "                                 kernel_size=3, padding=0)\n",
        "    self.act10 = torch.nn.ReLU6()\n",
        "    self.dropout9 = torch.nn.Dropout(p=dropout_rate)\n",
        "    self.conv10 = torch.nn.Conv2d(in_channels=512, out_channels=512,\n",
        "                                 kernel_size=3, padding=0)\n",
        "    self.act11 = torch.nn.ReLU6()\n",
        "    self.dropout10 = torch.nn.Dropout(p=dropout_rate)\n",
        "    self.pool2 = torch.nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "    self.act12 = torch.nn.ReLU6()\n",
        "\n",
        "    self.fc1 = torch.nn.Linear(5 * 5 * 512, 1000)\n",
        "    self.act13 = torch.nn.ReLU6()\n",
        "    self.fc2 = torch.nn.Linear(1000, 1000)\n",
        "    self.act14 = torch.nn.ReLU6()\n",
        "    self.fc3 = torch.nn.Linear(1000, 35)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1_a(x)\n",
        "    x = self.act1_a(x)\n",
        "    x = self.conv2_a(x)\n",
        "    x = self.act2_a(x)\n",
        "    x = self.conv3_a(x)\n",
        "    x = self.act3_a(x)\n",
        "    x = self.conv4_a(x)\n",
        "    x = self.act4_a(x)\n",
        "    x = self.conv5_a(x)\n",
        "    x = self.act5_a(x)\n",
        "\n",
        "    x = self.conv1(x)\n",
        "    x = self.act1(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.act2(x)\n",
        "    x = self.conv3(x)\n",
        "    x = self.act3(x)\n",
        "    x = self.conv4(x)\n",
        "    x = self.act4(x)\n",
        "    x = self.conv5(x)\n",
        "    x = self.act5(x)\n",
        "    x = self.pool1(x)\n",
        "    x = self.act6(x)\n",
        "    skip_connection_1 = x\n",
        "\n",
        "    x = self.conv6(x)\n",
        "    x = self.act7(x)\n",
        "    x = self.conv7(x)\n",
        "    x = self.act8(x)\n",
        "    x = self.conv8(x)\n",
        "    x = self.act9(x)\n",
        "    x = torch.add(x, skip_connection_1)\n",
        "    x = self.conv9(x)\n",
        "    x = self.act10(x)\n",
        "    x = self.conv10(x)\n",
        "    x = self.act11(x)\n",
        "    x = self.pool2(x)\n",
        "    x = self.act12(x)\n",
        "\n",
        "    x = x.view(x.size(0), x.size(1) * x.size(2) * x.size(3))\n",
        "\n",
        "    x = self.fc1(x)\n",
        "    x = self.act13(x)\n",
        "    x = self.fc2(x)\n",
        "    x = self.act14(x)\n",
        "    x = self.fc3(x)\n",
        "\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_list = []\n",
        "for experiments in range(1):\n",
        "  device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "  lenet = LeNetOneBlock(0.1).to(device)\n",
        "  loss = torch.nn.CrossEntropyLoss()\n",
        "  optimizer = AdaBound(lenet.parameters(), lr= 0.001, gamma = 1e-5, eps = 1e-6)\n",
        "  batch_size = 1120\n",
        "  for i in range(64):\n",
        "    order = np.random.permutation(len(train_data))\n",
        "    for start_index in range(0, len(train_data), batch_size):\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      batch_indexes = order[start_index:start_index + batch_size]\n",
        "      train_batch = train_data[batch_indexes].to(device)\n",
        "      label_batch = train_labels[batch_indexes].to(device)\n",
        "\n",
        "      preds = lenet.forward(train_batch)\n",
        "\n",
        "      loss_value = loss(preds, label_batch)\n",
        "      loss_value.backward()\n",
        "\n",
        "      optimizer.step()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    preds = lenet.forward(test_data)\n",
        "    accuracy = (preds.argmax(dim=1) == test_labels).float().mean().data.cpu()\n",
        "    accuracy_list.append(accuracy)\n",
        "    print(accuracy)\n",
        "    print(\"-----\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "_-OjQSlaQWoR",
        "outputId": "5af1bd7c-de56-4866-86fd-3e48240d809d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-cc2b867f55eb>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m       \u001b[0mlabel_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_indexes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m       \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlenet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-ec54cbef9754>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    454\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 456\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    457\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 858.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 495.06 MiB is free. Process 5417 has 14.26 GiB memory in use. Of the allocated memory 14.00 GiB is allocated by PyTorch, and 143.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LeNetTwoBlock(torch.nn.Module):\n",
        "  def __init__(self, dropout_rate):\n",
        "    super(LeNetTwoBlock, self).__init__()\n",
        "\n",
        "#beginning of the first block\n",
        "    self.conv1_a = torch.nn.Conv2d(in_channels=1, out_channels=2,\n",
        "                                 kernel_size=3, padding=1)\n",
        "    self.act1_a = torch.nn.ReLU6()\n",
        "    self.dropout1_a = torch.nn.Dropout(p=dropout_rate)\n",
        "    self.conv2_a = torch.nn.Conv2d(in_channels=2, out_channels=4,\n",
        "                                 kernel_size=3, padding=1)\n",
        "\n",
        "    self.act2_a = torch.nn.ReLU6()\n",
        "    self.dropout2_a = torch.nn.Dropout(p=dropout_rate)\n",
        "    self.conv3_a = torch.nn.Conv2d(in_channels=4, out_channels=8,\n",
        "                                 kernel_size=3, padding=1)\n",
        "    self.act3_a = torch.nn.ReLU6()\n",
        "    self.dropout3_a = torch.nn.Dropout(p=dropout_rate)\n",
        "    self.conv4_a = torch.nn.Conv2d(in_channels=8, out_channels=16,\n",
        "                                 kernel_size=3, padding=1)\n",
        "    self.act4_a = torch.nn.ReLU6()\n",
        "    self.dropout4_a = torch.nn.Dropout(p=dropout_rate)\n",
        "    self.conv5_a = torch.nn.Conv2d(in_channels=16, out_channels=32,\n",
        "                                 kernel_size=3, padding=1)\n",
        "    self.act5_a = torch.nn.ReLU6()\n",
        "    self.dropout5_a = torch.nn.Dropout(p=dropout_rate)\n",
        "#end of the first block\n",
        "\n",
        "    self.conv1 = torch.nn.Conv2d(in_channels=32, out_channels=64,\n",
        "                                 kernel_size=3, padding=1)\n",
        "    self.act1 = torch.nn.ReLU6()\n",
        "    self.dropout1 = torch.nn.Dropout(p=dropout_rate)\n",
        "    self.conv2 = torch.nn.Conv2d(in_channels=64, out_channels=128,\n",
        "                                 kernel_size=3, padding=1)\n",
        "\n",
        "    self.act2 = torch.nn.ReLU6()\n",
        "    self.dropout2 = torch.nn.Dropout(p=dropout_rate)\n",
        "    self.conv3 = torch.nn.Conv2d(in_channels=128, out_channels=256,\n",
        "                                 kernel_size=3, padding=1)\n",
        "    self.act3 = torch.nn.ReLU6()\n",
        "    self.dropout3 = torch.nn.Dropout(p=dropout_rate)\n",
        "    self.conv4 = torch.nn.Conv2d(in_channels=256, out_channels=512,\n",
        "                                 kernel_size=3, padding=1)\n",
        "    self.act4 = torch.nn.ReLU6()\n",
        "    self.dropout4 = torch.nn.Dropout(p=dropout_rate)\n",
        "    self.conv5 = torch.nn.Conv2d(in_channels=512, out_channels=512,\n",
        "                                 kernel_size=3, padding=1)\n",
        "    self.act5 = torch.nn.ReLU6()\n",
        "    self.dropout5 = torch.nn.Dropout(p=dropout_rate)\n",
        "    self.pool1 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    self.act6 = torch.nn.ReLU6()\n",
        "\n",
        "#beginning of the second block\n",
        "    self.conv1_b = torch.nn.Conv2d(in_channels=512, out_channels=512,\n",
        "                                 kernel_size=3, padding=1)\n",
        "    self.act1_b = torch.nn.ReLU6()\n",
        "    self.dropout1_b = torch.nn.Dropout(p=dropout_rate)\n",
        "    self.conv2_b = torch.nn.Conv2d(in_channels=512, out_channels=512,\n",
        "                                 kernel_size=3, padding=1)\n",
        "    self.act2_b= torch.nn.ReLU6()\n",
        "    self.dropout2_b = torch.nn.Dropout(p=dropout_rate)\n",
        "    self.conv3_b = torch.nn.Conv2d(in_channels=512, out_channels=512,\n",
        "                                 kernel_size=3, padding=1)\n",
        "    self.act3_b = torch.nn.ReLU6()\n",
        "    self.dropout3_b = torch.nn.Dropout(p=dropout_rate)\n",
        "    self.conv4_b = torch.nn.Conv2d(in_channels=512, out_channels=512,\n",
        "                                 kernel_size=3, padding=1)\n",
        "    self.act4_b = torch.nn.ReLU6()\n",
        "    self.dropout4_b = torch.nn.Dropout(p=dropout_rate)\n",
        "    self.conv5_b = torch.nn.Conv2d(in_channels=512, out_channels=512,\n",
        "                                 kernel_size=3, padding=1)\n",
        "    self.act5_a = torch.nn.ReLU6()\n",
        "    self.dropout5_a = torch.nn.Dropout(p=dropout_rate)\n",
        "#end of the second block\n",
        "\n",
        "    self.conv6 = torch.nn.Conv2d(in_channels=512, out_channels=512,\n",
        "                                 kernel_size=3, padding=1)\n",
        "    self.act7 = torch.nn.ReLU6()\n",
        "    self.dropout6 = torch.nn.Dropout(p=dropout_rate)\n",
        "    self.conv7 = torch.nn.Conv2d(in_channels=512, out_channels=512,\n",
        "                                 kernel_size=3, padding=1)\n",
        "    self.act8 = torch.nn.ReLU6()\n",
        "    self.dropout7 = torch.nn.Dropout(p=dropout_rate)\n",
        "    self.conv8 = torch.nn.Conv2d(in_channels=512, out_channels=512,\n",
        "                                 kernel_size=3, padding=1)\n",
        "    self.act9 = torch.nn.ReLU6()\n",
        "    self.dropout8 = torch.nn.Dropout(p=dropout_rate)\n",
        "    self.conv9 = torch.nn.Conv2d(in_channels=512, out_channels=512,\n",
        "                                 kernel_size=3, padding=0)\n",
        "    self.act10 = torch.nn.ReLU6()\n",
        "    self.dropout9 = torch.nn.Dropout(p=dropout_rate)\n",
        "    self.conv10 = torch.nn.Conv2d(in_channels=512, out_channels=512,\n",
        "                                 kernel_size=3, padding=0)\n",
        "    self.act11 = torch.nn.ReLU6()\n",
        "    self.dropout10 = torch.nn.Dropout(p=dropout_rate)\n",
        "    self.pool2 = torch.nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "    self.act12 = torch.nn.ReLU6()\n",
        "\n",
        "    self.fc1 = torch.nn.Linear(5 * 5 * 512, 1000)\n",
        "    self.act13 = torch.nn.ReLU6()\n",
        "    self.fc2 = torch.nn.Linear(1000, 1000)\n",
        "    self.act14 = torch.nn.ReLU6()\n",
        "    self.fc3 = torch.nn.Linear(1000, 35)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1_a(x)\n",
        "    x = self.act1_a(x)\n",
        "    x = self.conv2_a(x)\n",
        "    x = self.act2_a(x)\n",
        "    x = self.conv3_a(x)\n",
        "    x = self.act3_a(x)\n",
        "    x = self.conv4_a(x)\n",
        "    x = self.act4_a(x)\n",
        "    x = self.conv5_a(x)\n",
        "    x = self.act5_a(x)\n",
        "\n",
        "    x = self.conv1(x)\n",
        "    x = self.act1(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.act2(x)\n",
        "    x = self.conv3(x)\n",
        "    x = self.act3(x)\n",
        "    x = self.conv4(x)\n",
        "    x = self.act4(x)\n",
        "    x = self.conv5(x)\n",
        "    x = self.act5(x)\n",
        "    x = self.pool1(x)\n",
        "    x = self.act6(x)\n",
        "    skip_connection_1 = x\n",
        "\n",
        "    x = self.conv1_b(x)\n",
        "    x = self.act1_b(x)\n",
        "    x = self.conv2_b(x)\n",
        "    x = self.act2_b(x)\n",
        "    x = self.conv3_b(x)\n",
        "    x = self.act3_b(x)\n",
        "    x = self.conv4_b(x)\n",
        "    x = self.act4_b(x)\n",
        "    x = self.conv5_b(x)\n",
        "    x = self.act5_b(x)\n",
        "\n",
        "    x = self.conv6(x)\n",
        "    x = self.act7(x)\n",
        "    x = self.conv7(x)\n",
        "    x = self.act8(x)\n",
        "    x = self.conv8(x)\n",
        "    x = self.act9(x)\n",
        "    x = torch.add(x, skip_connection_1)\n",
        "    x = self.conv9(x)\n",
        "    x = self.act10(x)\n",
        "    x = self.conv10(x)\n",
        "    x = self.act11(x)\n",
        "    x = self.pool2(x)\n",
        "    x = self.act12(x)\n",
        "\n",
        "    x = x.view(x.size(0), x.size(1) * x.size(2) * x.size(3))\n",
        "\n",
        "    x = self.fc1(x)\n",
        "    x = self.act13(x)\n",
        "    x = self.fc2(x)\n",
        "    x = self.act14(x)\n",
        "    x = self.fc3(x)\n",
        "\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "lNdu7t9AhQD_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_list = []\n",
        "for experiments in range(1):\n",
        "  device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "  lenet = LeNetTwoBlock(0.1).to(device)\n",
        "  loss = torch.nn.CrossEntropyLoss()\n",
        "  optimizer = AdaBound(lenet.parameters(), lr= 0.001, gamma = 1e-5, eps = 1e-6)\n",
        "  batch_size = 1120\n",
        "  for i in range(64):\n",
        "    order = np.random.permutation(len(train_data))\n",
        "    for start_index in range(0, len(train_data), batch_size):\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      batch_indexes = order[start_index:start_index + batch_size]\n",
        "      train_batch = train_data[batch_indexes].to(device)\n",
        "      label_batch = train_labels[batch_indexes].to(device)\n",
        "\n",
        "      preds = lenet.forward(train_batch)\n",
        "\n",
        "      loss_value = loss(preds, label_batch)\n",
        "      loss_value.backward()\n",
        "\n",
        "      optimizer.step()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    preds = lenet.forward(test_data)\n",
        "    accuracy = (preds.argmax(dim=1) == test_labels).float().mean().data.cpu()\n",
        "    accuracy_list.append(accuracy)\n",
        "    print(accuracy)\n",
        "    print(\"-----\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "Oe5J-IT4jHuh",
        "outputId": "f9693d8d-664d-4297-e694-05843c5d9a1a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-bbdfebbbc0fb>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m       \u001b[0mlabel_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_indexes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m       \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlenet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-2a12a4eb27fa>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m     \u001b[0mskip_connection_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhardtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mhardtanh\u001b[0;34m(input, min_val, max_val, inplace)\u001b[0m\n\u001b[1;32m   1520\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhardtanh_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1521\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1522\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhardtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1523\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.68 GiB. GPU 0 has a total capacty of 14.75 GiB of which 927.06 MiB is free. Process 5417 has 13.84 GiB memory in use. Of the allocated memory 12.22 GiB is allocated by PyTorch, and 1.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    }
  ]
}